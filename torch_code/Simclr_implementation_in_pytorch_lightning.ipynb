{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simclr implementation in pytorch lightning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTtZvzLyFJ9C"
      },
      "source": [
        "# SimCLR implementation in PyTorch Lightning\n",
        "This is the matching colab for the implementation of SimCLR in PyTorch Lightning.\n",
        "\n",
        "List of full videos is here:    \n",
        "\n",
        "https://www.youtube.com/playlist?list=PLaMu-SDt_RB4k8VXiB3hOdsn0Y3GoXo1k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zna_bcK-dtvT"
      },
      "source": [
        "%%capture\n",
        "! pip install git+https://github.com/PytorchLightning/pytorch-lightning-bolts.git@master --upgrade\n",
        "! pip install pytorch-lightning==0.9.1rc1 --upgrade"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nBEjY_nTis5"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from typing import Optional"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2hkbQR2TtP-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "e46b318d-5079-4ba1-b943-83997bffc12b"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.callbacks import LearningRateLogger\n",
        "\n",
        "from pl_bolts.models.self_supervised.resnets import resnet50_bn\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from pl_bolts.datamodules import CIFAR10DataModule, STL10DataModule, ImagenetDataModule\n",
        "from pl_bolts.metrics import mean, accuracy\n",
        "\n",
        "from pl_bolts.models.self_supervised.evaluator import Flatten\n",
        "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization, stl10_normalization\n",
        "from pl_bolts.transforms.dataset_normalizations import imagenet_normalization\n",
        "from pl_bolts.optimizers import LARSWrapper"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b0772fbb8aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLearningRateLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpl_bolts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_supervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet50_bn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'LearningRateLogger'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZjswK4oT4qJ"
      },
      "source": [
        "class SimCLRTrainDataTransform(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_height: int = 224,\n",
        "        gaussian_blur: bool = False,\n",
        "        jitter_strength: float = 1.,\n",
        "        normalize: Optional[transforms.Normalize] = None\n",
        "    ) -> None:\n",
        "\n",
        "        self.jitter_strength = jitter_strength\n",
        "        self.input_height = input_height\n",
        "        self.gaussian_blur = gaussian_blur\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            0.8 * self.jitter_strength,\n",
        "            0.8 * self.jitter_strength,\n",
        "            0.8 * self.jitter_strength,\n",
        "            0.2 * self.jitter_strength\n",
        "        )\n",
        "\n",
        "        data_transforms = [\n",
        "            transforms.RandomResizedCrop(size=self.input_height),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomApply([self.color_jitter], p=0.8),\n",
        "            transforms.RandomGrayscale(p=0.2)\n",
        "        ]\n",
        "\n",
        "        if self.gaussian_blur:\n",
        "            data_transforms.append(GaussianBlur(kernel_size=int(0.1 * self.input_height, p=0.5)))\n",
        "\n",
        "        data_transforms.append(transforms.ToTensor())\n",
        "\n",
        "        if self.normalize:\n",
        "            data_transforms.append(normalize)\n",
        "\n",
        "        self.train_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        transform = self.train_transform\n",
        "\n",
        "        xi = transform(sample)\n",
        "        xj = transform(sample)\n",
        "\n",
        "        return xi, xj\n",
        "\n",
        "\n",
        "class SimCLREvalDataTransform(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_height: int = 224,\n",
        "        normalize: Optional[transforms.Normalize] = None\n",
        "    ):\n",
        "        self.input_height = input_height\n",
        "        self.normalize = normalize\n",
        "\n",
        "        data_transforms = [\n",
        "            transforms.Resize(self.input_height),\n",
        "            transforms.ToTensor()\n",
        "        ]\n",
        "\n",
        "        if self.normalize:\n",
        "            data_transforms.append(normalize)\n",
        "\n",
        "        self.test_transform = transforms.Compose(data_transforms)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        transform = self.test_transform\n",
        "\n",
        "        xi = transform(sample)\n",
        "        xj = transform(sample)\n",
        "\n",
        "        return xi, xj\n",
        "\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, p=0.5, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < self.p:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBNm6bbDT9J3"
      },
      "source": [
        "def nt_xent_loss(out_1, out_2, temperature):\n",
        "    out = torch.cat([out_1, out_2], dim=0)\n",
        "    n_samples = len(out)\n",
        "\n",
        "    # Full similarity matrix\n",
        "    cov = torch.mm(out, out.t().contiguous())\n",
        "    sim = torch.exp(cov / temperature)\n",
        "\n",
        "    mask = ~torch.eye(n_samples, device=sim.device).bool()\n",
        "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
        "\n",
        "    # Positive similarity\n",
        "    pos = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
        "    pos = torch.cat([pos, pos], dim=0)\n",
        "\n",
        "    loss = -torch.log(pos / neg).mean()\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JOQBJplT_mw"
      },
      "source": [
        "class Projection(nn.Module):\n",
        "    def __init__(self, input_dim=2048, hidden_dim=2048, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            Flatten(),\n",
        "            nn.Linear(self.input_dim, self.hidden_dim, bias=True),\n",
        "            nn.BatchNorm1d(self.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dim, self.output_dim, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return F.normalize(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbR-Na4NUK6G"
      },
      "source": [
        "class SimCLR(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 batch_size,\n",
        "                 num_samples,\n",
        "                 warmup_epochs=10,\n",
        "                 lr=1e-4,\n",
        "                 opt_weight_decay=1e-6,\n",
        "                 loss_temperature=0.5,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            batch_size: the batch size\n",
        "            num_samples: num samples in the dataset\n",
        "            warmup_epochs: epochs to warmup the lr for\n",
        "            lr: the optimizer learning rate\n",
        "            opt_weight_decay: the optimizer weight decay\n",
        "            loss_temperature: the loss temperature\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.nt_xent_loss = nt_xent_loss\n",
        "        self.encoder = self.init_encoder()\n",
        "\n",
        "        # h -> || -> z\n",
        "        self.projection = Projection()\n",
        "\n",
        "    def init_encoder(self):\n",
        "        encoder = resnet50_bn(return_all_feature_maps=False)\n",
        "\n",
        "        # when using cifar10, replace the first conv so image doesn't shrink away\n",
        "        encoder.conv1 = nn.Conv2d(\n",
        "            3, 64,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        return encoder\n",
        "\n",
        "    def exclude_from_wt_decay(self, named_params, weight_decay, skip_list=['bias', 'bn']):\n",
        "        params = []\n",
        "        excluded_params = []\n",
        "\n",
        "        for name, param in named_params:\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "            elif any(layer_name in name for layer_name in skip_list):\n",
        "                excluded_params.append(param)\n",
        "            else:\n",
        "                params.append(param)\n",
        "\n",
        "        return [\n",
        "            {'params': params, 'weight_decay': weight_decay},\n",
        "            {'params': excluded_params, 'weight_decay': 0.}\n",
        "        ]\n",
        "\n",
        "    def setup(self, stage):\n",
        "        global_batch_size = self.trainer.world_size * self.hparams.batch_size\n",
        "        self.train_iters_per_epoch = self.hparams.num_samples // global_batch_size\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TRICK 1 (Use lars + filter weights)\n",
        "        # exclude certain parameters\n",
        "        parameters = self.exclude_from_wt_decay(\n",
        "            self.named_parameters(),\n",
        "            weight_decay=self.hparams.opt_weight_decay\n",
        "        )\n",
        "\n",
        "        optimizer = LARSWrapper(Adam(parameters, lr=self.hparams.lr))\n",
        "\n",
        "        # Trick 2 (after each step)\n",
        "        self.hparams.warmup_epochs = self.hparams.warmup_epochs * self.train_iters_per_epoch\n",
        "        max_epochs = self.trainer.max_epochs * self.train_iters_per_epoch\n",
        "\n",
        "        linear_warmup_cosine_decay = LinearWarmupCosineAnnealingLR(\n",
        "            optimizer,\n",
        "            warmup_epochs=self.hparams.warmup_epochs,\n",
        "            max_epochs=max_epochs,\n",
        "            warmup_start_lr=0,\n",
        "            eta_min=0\n",
        "        )\n",
        "\n",
        "        scheduler = {\n",
        "            'scheduler': linear_warmup_cosine_decay,\n",
        "            'interval': 'step',\n",
        "            'frequency': 1\n",
        "        }\n",
        "\n",
        "        return [optimizer], [scheduler]\n",
        "\n",
        "    def forward(self, x):\n",
        "        if isinstance(x, list):\n",
        "            x = x[0]\n",
        "\n",
        "        result = self.encoder(x)\n",
        "        if isinstance(result, list):\n",
        "            result = result[-1]\n",
        "        return result\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.shared_step(batch, batch_idx)\n",
        "\n",
        "        result = pl.TrainResult(minimize=loss)\n",
        "        result.log('train_loss', loss, on_epoch=True)\n",
        "        return result\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.shared_step(batch, batch_idx)\n",
        "\n",
        "        result = pl.EvalResult(checkpoint_on=loss)\n",
        "        result.log('avg_val_loss', loss)\n",
        "        return result\n",
        "\n",
        "    def shared_step(self, batch, batch_idx):\n",
        "        (img1, img2), y = batch\n",
        "\n",
        "        # ENCODE\n",
        "        # encode -> representations\n",
        "        # (b, 3, 32, 32) -> (b, 2048, 2, 2)\n",
        "        h1 = self.encoder(img1)\n",
        "        h2 = self.encoder(img2)\n",
        "\n",
        "        # the bolts resnets return a list of feature maps\n",
        "        if isinstance(h1, list):\n",
        "            h1 = h1[-1]\n",
        "            h2 = h2[-1]\n",
        "\n",
        "        # PROJECT\n",
        "        # img -> E -> h -> || -> z\n",
        "        # (b, 2048, 2, 2) -> (b, 128)\n",
        "        z1 = self.projection(h1)\n",
        "        z2 = self.projection(h2)\n",
        "\n",
        "        loss = self.nt_xent_loss(z1, z2, self.hparams.loss_temperature)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kLxbIg2dScf"
      },
      "source": [
        "import os\n",
        "from pl_bolts.callbacks.self_supervised import SSLOnlineEvaluator\n",
        "\n",
        "# init callbacks\n",
        "def to_device(batch, device):\n",
        "    (img1, _), y = batch\n",
        "    img1 = img1.to(device)\n",
        "    y = y.to(device)\n",
        "    return img1, y\n",
        "\n",
        "online_finetuner = SSLOnlineEvaluator(z_dim=2048 * 2 * 2, num_classes = 10)\n",
        "online_finetuner.to_device = to_device\n",
        "\n",
        "lr_logger = LearningRateLogger()\n",
        "\n",
        "callbacks = [online_finetuner, lr_logger]\n",
        "\n",
        "# pick data\n",
        "cifar_height = 32\n",
        "batch_size = 8\n",
        "num_samples = 32\n",
        "\n",
        "# init data\n",
        "dm = CIFAR10DataModule(os.getcwd(), num_workers=0, batch_size=batch_size)\n",
        "dm.train_transforms = SimCLRTrainDataTransform(cifar_height)\n",
        "dm.val_transforms = SimCLREvalDataTransform(cifar_height)\n",
        "\n",
        "# realize the data\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "\n",
        "train_samples = len(dm.train_dataloader())\n",
        "\n",
        "model = SimCLR(batch_size=batch_size, num_samples=train_samples)\n",
        "trainer = pl.Trainer(callbacks=callbacks, progress_bar_refresh_rate=10, gpus=1)\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K0O1a8chCOt"
      },
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67u3AZxRlcym"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}